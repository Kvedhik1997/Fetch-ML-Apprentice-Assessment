{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "396db488-dc50-42b8-840e-9af2b6e27021",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83f211bb-44fe-47c3-adda-e657135b0b7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Multi-Task Model\n",
    "class MultiTaskSentenceTransformer(nn.Module):\n",
    "    def __init__(self, model_name=\"distilbert-base-uncased\", num_classes_task_a=3, num_classes_task_b=4):\n",
    "        super(MultiTaskSentenceTransformer, self).__init__()\n",
    "        # Load the pre-trained transformer model and tokenizer\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "        self.model = DistilBertModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Task A: Sentence Classification (Positive, Negative, Neutral)\n",
    "        self.task_a_classifier = nn.Linear(self.model.config.hidden_size, num_classes_task_a)\n",
    "        \n",
    "        # Task B: Sentiment Analysis (Happy, Sad, Angry, Neutral)\n",
    "        self.task_b_classifier = nn.Linear(self.model.config.hidden_size, num_classes_task_b)\n",
    "        \n",
    "    def forward(self, sentences):\n",
    "        # Tokenize the sentences\n",
    "        inputs = self.tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        \n",
    "        # Forward pass through the transformer model\n",
    "        outputs = self.model(**inputs)\n",
    "        \n",
    "        # Extract the last hidden state (token embeddings)\n",
    "        token_embeddings = outputs.last_hidden_state \n",
    "        \n",
    "        # Apply mean pooling (mean of token embeddings across the sequence length dimension)\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "        masked_token_embeddings = token_embeddings * attention_mask.unsqueeze(-1)\n",
    "        sentence_embeddings = masked_token_embeddings.sum(dim=1) / attention_mask.sum(dim=1, keepdim=True)\n",
    "        \n",
    "        # Task A: Sentence Classification\n",
    "        task_a_output = self.task_a_classifier(sentence_embeddings)\n",
    "        \n",
    "        # Task B: Sentiment Analysis\n",
    "        task_b_output = self.task_b_classifier(sentence_embeddings)\n",
    "        \n",
    "        return task_a_output, task_b_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea0d2ffe-1f58-47ca-b880-03973ddefe54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Layer wise learning rate implementation\n",
    "def layerwise_optimizer(model, lower_lr=1e-6, middle_lr=1e-5, higher_lr=1e-4):\n",
    "    param_groups = []\n",
    "    \n",
    "    # Lower layers of the transformer\n",
    "    initial_layers = model.model.embeddings.parameters()\n",
    "    param_groups.append({\"params\": initial_layers, \"lr\": lower_lr})\n",
    "    \n",
    "    # Middle layers of the transformer\n",
    "    middle_layers = model.model.transformer.layer[:6].parameters()  # Use the first 6 layers of DistilBERT\n",
    "    param_groups.append({\"params\": middle_layers, \"lr\": middle_lr})\n",
    "    \n",
    "    # Upper layers of the transformer (task-specific)\n",
    "    upper_layers = model.model.transformer.layer[6:].parameters()  # Use the last layers of DistilBERT\n",
    "    param_groups.append({\"params\": upper_layers, \"lr\": higher_lr})\n",
    "    \n",
    "    # Task-specific heads\n",
    "    task_a_head = model.task_a_classifier.parameters()\n",
    "    task_b_head = model.task_b_classifier.parameters()\n",
    "    param_groups.append({\"params\": task_a_head, \"lr\": higher_lr})\n",
    "    param_groups.append({\"params\": task_b_head, \"lr\": higher_lr})\n",
    "    \n",
    "    # Create the optimizer using AdamW\n",
    "    optimizer = AdamW(param_groups)\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8df0b96d-d157-4d53-9597-002bfc911730",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = MultiTaskSentenceTransformer(model_name=\"distilbert-base-uncased\", num_classes_task_a=3, num_classes_task_b=4)\n",
    "optimizer = layerwise_optimizer(model, lower_lr=1e-6, middle_lr=1e-5, higher_lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b76a6a-f8d0-49d6-86e4-bce586e48712",
   "metadata": {},
   "source": [
    "Layer wise learning rate implementation can optimize computational costs while retaining accuracy. By using a pretrained model, such as distilbert-base-uncased, as the backbone of the model, we can decrease the learning rate for the early layers of the model. These layers capture general language features, which have already been trained into the distilbert-base-uncased model. The middle layers capture more task relevant features, so a higher learning rate is used. The final layers consist of the task specific heads, which are optimized by using the highest learning rate. By implementing layerwise learning rates, we can save computational costs by avoiding retraining the model with redundant information. Learning rate is increased only in layers that adapt the model to the specific tasks at hand. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
